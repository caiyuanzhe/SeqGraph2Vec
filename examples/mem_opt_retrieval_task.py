# -*- coding: utf-8 -*-
import sys
sys.path.extend(['.', '..'])
import os
import time
from gensim.models import KeyedVectors
from Bio import SeqIO
import re
from tqdm import tqdm
import gc

from util.faiss_getprecision import create_index
from util.faiss_getprecision import precision
from util.perf_tools import Tee

from src.generators import seq2segs
from src.generators import seg2sentence, seq2sentence
from src.generators import extract_seg
from src.generators import parse_seq

from src.seqgraph2vec import SeqGraph2Vec

import multiprocessing
from src.cython_function import fast_compute_edge_weight, extract_kmer
from collections import Counter
import networkx as nx

def parse_seq(path_to_input: str):
    """ Return a list containing DNA seqment(s) captured in fna file(s)."""
    seq_files = list()
    for input_file_dir in path_to_input:
        print(input_file_dir)
        for root, dirs, files in os.walk(input_file_dir):
            for file in files:
                if file.endswith('.fna'):
                    seq_files.append(os.path.join(root, file))
    seqs = list()
    for seq_file in seq_files:
        for seq_record in SeqIO.parse(seq_file, 'fasta'):
            seq = re.sub('[^ACGTacgt]+', '', str(seq_record.seq))
            seqs.append(seq.upper())

    print('There are ' + str(len(seqs)) + ' seqs')

    return seqs


class KMerEmbeddings:

    def __init__(
        self,
        p: float,
        q: float,
        dimensions: int,  
        workers: int,
        path_to_edg_list_file: str,
        kmer_vec_output_dir: str,
        pgr: dict,
    ):
        self.p = p
        self.q = q
        self.workers = workers
        self.dimensions = dimensions
        self.path_to_edg_list_file = path_to_edg_list_file
        self.kmer_vec_output_dir = kmer_vec_output_dir
        self.pgr = pgr

    def train(self):
        """ Obtain the k-mer embedding. """
        print(self.path_to_edg_list_file)
        clf = SeqGraph2Vec(p=self.p, q=self.q, workers=self.workers, dimensions=self.dimensions, pgr=self.pgr)
        clf.fit(
            path_to_edg_list_file=self.path_to_edg_list_file,
            path_to_embeddings_file=self.kmer_vec_output_dir + f"kmer-node2vec-embedding.txt",
        )
        

class SequenceEmbeddings:

    def __init__(
        self,
        mer: int,
        kmer2vec_file: str,
        seq_dir: str,
        segment_length: int,
        segment_number: int,
        segment_file: str,
        extracted_original_segment_file: str,
        extracted_subsegment_file: str,
        sequence_vec_output_dir: str,

    ):
        """ segment embeddings """
        self.mer = mer  
        self.kmer2vec_file = kmer2vec_file
        self.seqs = parse_seq([seq_dir])
        self.seg_vec_output_dir = sequence_vec_output_dir

        # the length and number of segments that we will split
        self.seg_length = segment_length
        self.seg_num = segment_number

        """ These files will be generated by the 'train' function."""
        self.seg_file = segment_file  # segments split from sequences
        # randomly choose 1k segments from seg_file and extract subsegments from the 1k segments
        self.extracted_orgseg_file = extracted_original_segment_file
        self.extracted_subseg_file = extracted_subsegment_file

    def segment_embeddings(self):
        """ Obtain segment embeddings with pre-trained k-mer embeddings. """

        # create segment.txt;
        # note that if an old one exists, we would not overwrite it.
        if not os.path.exists(self.seg_file):
            seq2segs(
                self.seqs,
                self.seg_length,
                self.seg_file,
            )

        with open(self.seg_file, 'r', encoding='utf-8') as fp:
            segs = [line.split('\n')[0] for line in fp.readlines()]
        """ 1. kmer fragmentation in DNA2Vec style """
        sentences = seg2sentence(segs, self.mer)  # Tokenize
        """ 2. kmer fragmentation in ProtVec style """
        # overlap = 5
        # sentences = seq2sentence(segs, overlap=overlap, mer=self.mer)  # 2D array

        from util.vectorizer import SeqVectorizer
        vecs = KeyedVectors.load_word2vec_format(self.kmer2vec_file)  # k-mer vectors

        clf = SeqVectorizer(vecs)
        clf.train(sentences)
        clf.save_embs_format(
            self.seg_vec_output_dir,
            f"{'SegmentVectors'}"
        )

    def subseg_embeddings(self):

        # Randomly extract sub-segments from self.seg_file
        if not os.path.exists(self.extracted_orgseg_file) and \
                not os.path.exists(self.extracted_subseg_file):  # prevent overwriting
            extract_seg(
                self.seg_file,
                self.seg_length,
                self.seg_num,
                self.extracted_subseg_file,
                self.extracted_orgseg_file,
            )

        # load subsegments
        with open(self.extracted_subseg_file, 'r', encoding='utf-8') as fp:
            subsegs = [line.split('\n')[0] for line in fp.readlines()]
        """ 1. kmer fragmentation in DNA2Vec style """
        sentences = seg2sentence(subsegs, self.mer)  # Tokenize
        """ 2. kmer fragmentation in ProtVec style """
        # overlap = 5
        # sentences = seq2sentence(subsegs, overlap=overlap, mer=self.mer)  # 2D array

        from util.vectorizer import SeqVectorizer
        vecs = KeyedVectors.load_word2vec_format(self.kmer2vec_file)  # k-mer2vec file

        clf = SeqVectorizer(vecs)
        clf.train(sentences, vector_size=128)
        clf.save_embs_format(
            self.seg_vec_output_dir,
            f"{'SubSegmentVectors'}"
        )

    def train(self):
        """ Generate four types of files:
            1、One file of segments.
            2、One file of randomly extracted subsegments.
            3、One file of randomly extracted segments from which subsegments originally come.
            4、Six files of vectors corresponding to segments/subsegments. 

        Note:
            Files of segments/subsegments are designed to be fed 
            into sequence retrieval task. See 'class SequenceRetrieval'.
        """
        self.segment_embeddings()
        self.subseg_embeddings()


class SequenceRetrieval:

    def __init__(
        self,
        segment_name_file: str,
        segment_vec_file: str,
        original_subsegment_name_fle: str,
        subsegment_vec_file: str,
        faiss_index_file: str,
        faiss_log: str,
        top_kn: int,
    ):
        """ Note: we use segments from which ramdomly selected subsegments 
            originally come to take the place of subsegments, in order to 
            make comparisons with segments in corpus . """
        self.segment_name_file = segment_name_file
        self.segment_vec_file = segment_vec_file
        self.original_subsegment_name_fle = original_subsegment_name_fle
        self.subsegment_vec_file = subsegment_vec_file

        self.faiss_index_file = faiss_index_file
        self.faiss_log = faiss_log
        self.top_kn = top_kn

    def train(
        self,
        dimension: int,
        index_method: str,
        vertex_connection: int,
        ef_search: int,
        ef_construction: int,
    ):
        """ Print the Top-K result for sequence retrieval task """
        logger = Tee(self.faiss_log)
        sys.stdout = logger

        if create_index(
            self.segment_vec_file,
            self.faiss_index_file,
            dimension,
            index_method,
            vertex_connection,
            ef_search,
            ef_construction,
        ):
            precision(
                self.subsegment_vec_file,
                self.original_subsegment_name_fle,
                self.segment_name_file,
                self.faiss_index_file,
                self.top_kn
            )


def kmer_embeddings(work_dir):
    clf = KMerEmbeddings(
        p=1.0,
        q=0.001,
        dimensions=128,
        workers=8,
        kmer_vec_output_dir=work_dir,
        path_to_edg_list_file=work_dir+'networkfile.edg',  # default setting
        pgr=pgr
    )
    clf.train()


def sequence_embeddings(work_dir):
    clf = SequenceEmbeddings(
        mer=8,  # consistent with the length k of pre-trained k-mers
        kmer2vec_file=work_dir+f"best-8mer-kmer-node2vec-embedding.txt",  # set this
        seq_dir=work_dir,
        segment_length=200,  # set this
        segment_number=2,  # set this
        segment_file=work_dir+'segment.txt',
        extracted_original_segment_file=work_dir+'extracted_org_segment.txt',
        extracted_subsegment_file=work_dir+'extracted_sub_segment.txt',
        sequence_vec_output_dir=work_dir,
    )
    clf.train()


def sequence_retrieval(work_dir):
    clf = SequenceRetrieval(
        segment_name_file=work_dir+'segment.txt',
        segment_vec_file=work_dir+'SegmentVectors.txt',  # set this
        original_subsegment_name_fle=work_dir+'extracted_org_segment.txt',
        subsegment_vec_file=work_dir+'SubSegmentVectors.txt',  # set this
        faiss_index_file=work_dir+'faiss-index-file',
        faiss_log=work_dir+'faiss-result.log',
        top_kn=1,
    )
    clf.train(
        dimension=128,  # set this, consistent with dimensionality of sequence embedding
        index_method='HNSW',
        vertex_connection=100,
        ef_search=2000,
        ef_construction=128,
    )


####
def compute_edge_weight(seqs, global_weight_dict, mer, process_id):
    weight_dict = fast_compute_edge_weight(seqs, mer)
    global_weight_dict[process_id] = weight_dict


def split_list(lst, n):
    k, m = divmod(len(lst), n)
    return list(lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))


if __name__ == '__main__':
    """
    Note: 
        Run functions !!!one by one!!!, since the next 
        function's input requires the previous function's output.
        Need to manually modify some file names in each Step.
    """
    
    ###### Step1: train the kmer embedding, you can skip this step if you have pre-trained kmer embedding. ######    
    """ Generate a kmer graph file """
    work_dir = '../data_dir/input/seq/small_data/'
    seq_dir = work_dir
    edge_list_path = work_dir + 'networkfile.edg'
    mer = 3
    dataprocess_workers = 8
    seq_file_num_to_load = 1
    
    seq_files = list()
    for input_file_dir in [work_dir]:
        print('input_file_dir', input_file_dir)
        for root, dirs, files in os.walk(input_file_dir):
            for file in files:
                if file.endswith('.fna'):
                    seq_files.append(os.path.join(root, file))
    

    all_edge_weight_dict = dict()
    for i in range(0, len(seq_files), seq_file_num_to_load):

        # Load 'seq_file_num_to_load' fna files in memory each time
        files = seq_files[i:i+seq_file_num_to_load]
        seqs = list()
        for f in files:
            for seq_record in SeqIO.parse(f, 'fasta'):
                seq = re.sub('[^ACGTacgt]+', '', str(seq_record.seq))
                seqs.append(seq.upper())

        manager = multiprocessing.Manager()  # multiprocess for computing edge weights
        edge_weight_dict = manager.dict()
        processes = []
        for i,partition_seqs in enumerate(split_list(seqs, n=dataprocess_workers)):
            p = multiprocessing.Process(
                target=compute_edge_weight,
                args=(partition_seqs, edge_weight_dict, mer, i)
            )
            processes.append(p)
            p.start()
        for p in processes:
            p.join()

        tmp = Counter({})  # merge multiple edge weight dictionaries into single one
        for i in range(0, dataprocess_workers):
            tmp.update(Counter(edge_weight_dict[i]))
        all_edge_weight_dict.update(dict(tmp))

        # memory recycling
        del tmp
        del edge_weight_dict
        del seqs
        del p
        del partition_seqs
        del processes
        del manager
        leak = gc.collect()
        print("gc.collect() returned", leak)

    edge_list = [(nodes[0], nodes[1], weight) for nodes, weight in all_edge_weight_dict.items()]
    with open(edge_list_path, 'w', encoding='utf-8') as edge_list_file:
        for edge_pair in edge_list:
            write_content = str(edge_pair[0]) + '\t' + str(edge_pair[1]) + '\t' + str(edge_pair[2]) + '\n'
            edge_list_file.write(write_content)
    
    graph = nx.DiGraph()
    for nodes, weight in all_edge_weight_dict.items():
        graph.add_edge(
            nodes[0], nodes[1],
            weight=weight
        )
    # print(graph.edges.data("weight"))
    pgr = nx.pagerank(graph, alpha=0.85)

    """ Use random walk sampling and Skip-Gram to learn kmer embedding """
    kmer_embeddings(work_dir=work_dir)  
    ###### END Step1 ######
 

    ###### Step2 ######
    # sequence_embeddings(work_dir='../data_dir/input/retrieval_test/')
    ###### End Step2 ######


    ###### Step3 ######
    # sequence_retrieval(work_dir='../data_dir/input/retrieval_test/')
    ###### End Step3 ######
